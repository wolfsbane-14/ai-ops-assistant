# AI Ops Assistant

An AI Operations Assistant that accepts a natural-language task, plans steps, ## Integrated APIs (Third-Party)

### 1. GitHub REST API
- **Base URL**: `https://api.github.com`
- **Endpoints Used**:
  - `/search/repositories` - Search for repositories
  - `/repos/{owner}/{repo}` - Get repository details
- **Authentication**: Optional GitHub token for higher rate limits
- **Tool**: `GitHubTool` in `ai_ops_assistant/tools/github_tool.py`

### 2. Open-Meteo Weather API
- **Base URL**: `https://api.open-meteo.com`
- **Endpoints Used**:
  - `/v1/search` - Geocoding for city location
  - `/v1/forecast` - Current weather data
- **Authentication**: None required (free service)
- **Tool**: `WeatherTool` in `ai_ops_assistant/tools/weather_tool.py`

## Example Prompts (3-5 Test Cases)

### Test via curl:

```bash
# 1. Simple weather query
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{"task":"Get weather in Paris"}'

# 2. Simple GitHub search
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{"task":"Find popular FastAPI repositories on GitHub"}'

# 3. Combined query (both APIs)
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{"task":"Find a GitHub repo about vector databases and tell me today's weather in Berlin"}'

# 4. Comparison task
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{"task":"Compare FastAPI and Flask repositories on GitHub by stars"}'

# 5. Multi-step query
curl -X POST http://127.0.0.1:8000/run \
  -H "Content-Type: application/json" \
  -d '{"task":"Get the weather in Tokyo and find a popular Python machine learning repository"}'
```

### Expected Response Format:

```json
{
  "result": {
    "answer": "Human-readable summary of the results",
    "data": {
      "key": "structured data from APIs"
    },
    "sources": ["GitHub API", "Open-Meteo API"]
  },
  "metadata": {
    "steps": [{"tool": "...", "input": {...}}],
    "tools_used": ["tool_name_1", "tool_name_2"]
  }
}
```(APIs),
and returns a structured answer. The system uses a multi-agent design (Planner, Executor,
Verifier) and integrates real third-party APIs.

## ✅ Requirements Coverage

- **Multi-agent design**: Planner → Executor → Verifier in `ai_ops_assistant/agents/`
- **LLM structured outputs**: Gemini with JSON schema validation via Pydantic
- **At least 2 real APIs**: GitHub REST API and Open-Meteo Weather API
- **End-to-end result**: `/run` endpoint returns complete structured response
- **No hardcoded responses**: All content dynamically generated by LLM + API data

## Setup Instructions (Localhost)

### Prerequisites
- Python 3.11+ installed
- Gemini API key (get free at https://aistudio.google.com/)

### Installation Steps

1. **Clone the repository** (or extract if provided as zip)
```bash
cd "trulymadly genai intern project"
```

2. **Create and activate a virtual environment**
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY
```

5. **Run the server**
```bash
uvicorn ai_ops_assistant.main:app --reload
```

The API will be available at `http://127.0.0.1:8000`

## Environment Variables

Create a `.env` file (copy from `.env.example`) with these required values:

| Variable | Required | Description | Default |
|----------|----------|-------------|---------|
| `GEMINI_API_KEY` | Yes | Your Gemini API key from Google AI Studio | - |
| `GEMINI_MODEL` | No | Gemini model to use | `gemini-1.5-flash` |
| `GITHUB_TOKEN` | No | GitHub personal access token (optional, for higher rate limits) | - |
| `ENABLE_CACHE` | No | Enable response caching to reduce API calls | `true` |
| `CACHE_TTL` | No | Cache time-to-live in seconds | `3600` |

**Note**: Open-Meteo API requires no API key (free service).

## Architecture Overview

### Agent Design (Multi-Agent System)

1. **Planner Agent** (`ai_ops_assistant/agents/planner.py`)
   - Receives natural language task from user
   - Uses LLM to generate structured JSON plan with tool calls
   - Selects appropriate tools based on task requirements
   - Returns ordered list of execution steps

2. **Executor Agent** (`ai_ops_assistant/agents/executor.py`)
   - Receives plan from Planner
   - Executes each step by calling the appropriate tool/API
   - Handles tool input normalization
   - Returns results from all tool executions

3. **Verifier Agent** (`ai_ops_assistant/agents/verifier.py`)
   - Validates completeness of execution results
   - Checks if all required data was obtained
   - Can suggest additional steps if data is missing
   - Formats final structured response for user

### LLM Integration (`ai_ops_assistant/llm/`)

- **Structured JSON Outputs**: All LLM calls use Pydantic schemas for validation
- **Caching**: Response cache reduces redundant API calls (configurable TTL)
- **Rate Limit Handling**: Exponential backoff retry logic for API quota limits
- **Robust Parsing**: Handles various JSON formats returned by Gemini

### Tools/APIs (`ai_ops_assistant/tools/`)

Each tool wraps a third-party API with error handling and result formatting.

## Integrated APIs

- **GitHub REST API**: Repository search + repo details.
- **Open-Meteo API**: Current weather by city (no API key required).

## Example Prompts

1. "Find the top 3 open-source LLM repos on GitHub and summarize them."
2. "What’s the weather in Mumbai and show a repo related to weather dashboards."
3. "Compare a popular FastAPI repo with a Flask repo (stars, description)."
4. "Get current weather in New York and list a GitHub repo for climate data."
5. "Find a GitHub repo about vector databases and tell me today’s weather in Berlin."

## Known Limitations / Tradeoffs

### Current Limitations:
1. **LLM Dependency**: Tool selection quality depends on LLM prompt interpretation
2. **Rate Limits**: 
   - Gemini free tier has quota limits (caching helps mitigate)
   - GitHub API: 60 requests/hour without token, 5000/hour with token
   - Open-Meteo: Shared rate limits on free tier
3. **No Persistent Storage**: Cache is in-memory only (cleared on restart)
4. **Sequential Execution**: Tools execute one at a time (no parallelization)
5. **Error Recovery**: Limited retry logic (3 attempts with exponential backoff)

### Design Tradeoffs:
- **Simplicity vs Features**: Focused on core requirements over advanced features
- **Caching**: Reduces API calls but may return stale data (1-hour TTL)
- **Verification Step**: Can be skipped for simple single-tool queries to reduce LLM calls
- **JSON Parsing**: Lenient parser handles various Gemini output formats but may accept malformed responses

### Improvements With More Time:
- Persistent cache (Redis/disk-based)
- Parallel tool execution for independent steps
- Cost tracking per request
- More comprehensive error handling and logging
- Additional APIs (News, Stock data, etc.)
- WebSocket support for streaming responses
- Admin UI for monitoring and debugging

## Project Structure

```
ai_ops_assistant/
├── agents/
│   ├── __init__.py
│   ├── planner.py      # Planner Agent
│   ├── executor.py     # Executor Agent
│   └── verifier.py     # Verifier Agent
├── llm/
│   ├── __init__.py
│   ├── client.py       # Gemini LLM client with retry logic
│   ├── schemas.py      # Pydantic schemas for structured outputs
│   └── cache.py        # In-memory response cache
├── tools/
│   ├── __init__.py
│   ├── github_tool.py  # GitHub API integration
│   └── weather_tool.py # Open-Meteo API integration
├── __init__.py
└── main.py             # FastAPI application entry point

requirements.txt        # Python dependencies
.env.example           # Environment variables template
README.md              # This file
```

## Running the Project

**Single command to start:**
```bash
uvicorn ai_ops_assistant.main:app --reload
```

**Alternative (production mode):**
```bash
uvicorn ai_ops_assistant.main:app --host 0.0.0.0 --port 8000
```

## Verification Steps

To verify the system works correctly:

1. **Start the server** (see above)
2. **Test health check**: `curl http://127.0.0.1:8000/docs` (FastAPI auto-docs)
3. **Run example queries** (see Example Prompts section)
4. **Check logs** for agent execution flow and API calls
5. **Verify response format** matches expected JSON structure

## License

This project is submitted as part of an internship assignment.
